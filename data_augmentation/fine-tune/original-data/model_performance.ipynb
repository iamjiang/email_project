{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c66e1f-7a70-464f-b74e-3f21cad90570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86358397-c48c-47a8-9c5d-aec7b9ab3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large_customized\"\n",
    "output_dir=os.path.join(os.getcwd(),model_name)\n",
    "df=pd.read_csv(os.path.join(output_dir,\"predictions.csv\"))\n",
    "true_y=df[\"True label\"].values\n",
    "pred_y=df[\"Predicted label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311bfa9d-6a37-4f1c-ac51-a7c26d5eb075",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(f\"\\n===========Test Set Performance===============\\n\")\n",
    "print()\n",
    "\n",
    "print(classification_report(true_y, pred_y))\n",
    "print()\n",
    "print()\n",
    "print(confusion_matrix(true_y, pred_y))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582ebd6-6f34-4057-a20a-082ec8fb132d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69555fac-6bc5-45cf-b8e9-dd4d9b618576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_table(table_name=\"metrics_test.txt\"):\n",
    "    Model_Type=[]\n",
    "    Total_Complaint=[]\n",
    "    False_Positive=[]\n",
    "    False_Negative=[]\n",
    "    Precision=[]\n",
    "    Recall=[]\n",
    "    F1_Score=[]\n",
    "    ROC_AUC=[]\n",
    "    PR_AUC=[]\n",
    "\n",
    "    with open(table_name,'r') as f:\n",
    "        for line in f:\n",
    "            Model_Type.append(str(line.split(\",\")[0]))\n",
    "            Total_Complaint.append(int(line.split(\",\")[1]))\n",
    "            False_Positive.append(int(line.split(\",\")[2]))\n",
    "            False_Negative.append(int(line.split(\",\")[3]))\n",
    "            Precision.append(float(line.split(\",\")[4]))\n",
    "            Recall.append(float(line.split(\",\")[5]))\n",
    "            F1_Score.append(float(line.split(\",\")[6]))\n",
    "            ROC_AUC.append(float(line.split(\",\")[7]))\n",
    "            PR_AUC.append(float(line.split(\",\")[8]))\n",
    "\n",
    "    metrics=pd.DataFrame({\"model_type\":Model_Type,\"total complaint #\":Total_Complaint,\"false_positive\":False_Positive,\"false_negative\":False_Negative,\\\n",
    "                         \"precision\":Precision,\"recall\":Recall,\"f1_score\":F1_Score,\"roc_auc\":ROC_AUC,\"pr_auc\":PR_AUC})\n",
    "    # metrics.drop_duplicates(subset=[\"model_type\",\"epoch\"],inplace=True)\n",
    "    # metrics.sort_values(by=['model_type','epoch'],inplace=True)       \n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def style_format(metrics, type=\"test set\"):\n",
    "    # metrics=metrics[metrics[\"model_type\"].apply(lambda x : x.split(\"-\")[0]==model.split(\"-\")[0])].reset_index(drop=True)\n",
    "    return metrics.style.format({\"total complaint #\":\"{:,}\",\"false_positive\":\"{:,}\",\"false_negative\":\"{:,}\", \"precision\":\"{:.2%}\", \"recall\":\"{:.2%}\", \\\n",
    "                                \"f1_score\":\"{:.2%}\", \"roc_auc\":\"{:.2%}\",\"pr_auc\":\"{:.2%}\"}) \\\n",
    "    .set_caption(f\"Performance Summary For {type} \") \\\n",
    "    .set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': [\n",
    "            ('color', 'red'),\n",
    "            ('font-size', '20px')\n",
    "        ]\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d972767-ab89-456d-a6e7-7d94e3ac2947",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large\"\n",
    "output_dir=os.path.join(os.getcwd(),model_name)\n",
    "pretrained_longformer=metric_table(table_name=os.path.join(output_dir,\"metrics_test.txt\"))\n",
    "pretrained_longformer=pretrained_longformer.tail(1)\n",
    "pretrained_longformer['model_type'] = pretrained_longformer['model_type'].replace(['longformer-large-4096'], 'pretrained-longformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be14d7c-c241-445b-886e-22ece2f4ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large_customized\"\n",
    "output_dir=os.path.join(os.getcwd(),model_name)\n",
    "customized_longformer=metric_table(table_name=os.path.join(output_dir,\"metrics_test.txt\"))\n",
    "customized_longformer=customized_longformer.tail(1)\n",
    "customized_longformer['model_type'] = customized_longformer['model_type'].replace(['longformer-large-4096'], 'customized-longformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8b129-315b-4f31-9ec0-03faeddbb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_test = pd.concat([pretrained_longformer, customized_longformer], axis=0)\n",
    "metric_test.drop_duplicates(subset=['model_type'],inplace=True)\n",
    "style_format(metric_test,  type=\"test set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa5154-6463-4037-8478-928b9f3acff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4924ab-261c-4e7a-a5c9-13beeeecd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score,average_precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve,precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e881bf-1c82-4b1e-bb5b-34f4f33683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_func(output_dir,title_name):\n",
    "    df=pd.read_csv(os.path.join(output_dir,\"predictions.csv\"))\n",
    "    true_y=df[\"True label\"].values\n",
    "    pred_y=df[\"Predicted label\"].values\n",
    "    confusion_matrix = metrics.confusion_matrix(true_y, pred_y)\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
    "    cm_display.plot(values_format=',')\n",
    "    plt.title(title_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03842616-ccba-4616-9ec1-a29dd19f080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_read(table_name):\n",
    "    with open(table_name,\"r\") as file:\n",
    "        true_y=[]\n",
    "        prob_y=[]\n",
    "        for line in file:\n",
    "            x,y,z=line.strip().split(',')\n",
    "            true_y.append(int(x))\n",
    "            prob_y.append(float(z)) \n",
    "    return true_y, prob_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d0b4f-7782-4343-859b-22df49d0b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large\"\n",
    "out_dir=os.path.join(os.getcwd(),model_name)\n",
    "confusion_matrix_func(out_dir,title_name=\"pretrained-longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4118dde-0477-4ef9-9919-f0e03e34d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large_customized\"\n",
    "out_dir=os.path.join(os.getcwd(),model_name)\n",
    "confusion_matrix_func(out_dir,title_name=\"customized-longformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babb29ed-caf2-4857-b757-bdc1d4600582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_rate_eval(logit,label,topk):\n",
    "    DF=pd.DataFrame(columns=[\"pred_score\",\"actual_label\"])\n",
    "    DF[\"pred_score\"]=logit\n",
    "    DF[\"actual_label\"]=label\n",
    "    DF.sort_values(by=\"pred_score\", ascending=False, inplace=True)\n",
    "    response_rate={}\n",
    "    for p in topk:\n",
    "        N=math.ceil(int(DF.shape[0]*p))\n",
    "        DF2=DF.nlargest(N,\"pred_score\",keep=\"first\")\n",
    "        response_rate[str(int(p*100))+\"%\"]=DF2.actual_label.sum()/DF2.shape[0]\n",
    "    return response_rate\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "def bar_plot(data, colors=None, total_width=0.8, single_width=1, legend=True,title=None,subtitle=None,axis_truncation=0.5):\n",
    "    \"\"\"Draws a bar plot with multiple bars per data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax : matplotlib.pyplot.axis\n",
    "        The axis we want to draw our plot on.\n",
    "\n",
    "    data: dictionary\n",
    "        A dictionary containing the data we want to plot. Keys are the names of the\n",
    "        data, the items is a list of the values.\n",
    "\n",
    "        Example:\n",
    "        data = {\n",
    "            \"x\":[1,2,3],\n",
    "            \"y\":[1,2,3],\n",
    "            \"z\":[1,2,3],\n",
    "        }\n",
    "\n",
    "    colors : array-like, optional\n",
    "        A list of colors which are used for the bars. If None, the colors\n",
    "        will be the standard matplotlib color cyle. (default: None)\n",
    "\n",
    "    total_width : float, optional, default: 0.8\n",
    "        The width of a bar group. 0.8 means that 80% of the x-axis is covered\n",
    "        by bars and 20% will be spaces between the bars.\n",
    "\n",
    "    single_width: float, optional, default: 1\n",
    "        The relative width of a single bar within a group. 1 means the bars\n",
    "        will touch eachother within a group, values less than 1 will make\n",
    "        these bars thinner.\n",
    "\n",
    "    legend: bool, optional, default: True\n",
    "        If this is set to true, a legend will be added to the axis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if colors where provided, otherwhise use the default color cycle\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize =(15, 8))\n",
    "    \n",
    "    if colors is None:\n",
    "        colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    \n",
    "    # Number of bars per group\n",
    "    n_bars = len(data)\n",
    "\n",
    "    # The width of a single bar\n",
    "    bar_width = total_width / n_bars\n",
    "\n",
    "    # List containing handles for the drawn bars, used for the legend\n",
    "    bars = []\n",
    "\n",
    "    # Iterate over all data\n",
    "    for i, (name, values) in enumerate(data.items()):\n",
    "        # The offset in x direction of that bar\n",
    "        x_offset = (i - n_bars / 2) * bar_width + bar_width / 2\n",
    "\n",
    "        # Draw a bar for every value of that type\n",
    "        for x, y in enumerate(values.values()):\n",
    "            bar = ax.bar(x + x_offset, y, width=bar_width * single_width, color=colors[i % len(colors)])\n",
    "\n",
    "        # Add a handle to the last drawn bar, which we'll need for the legend\n",
    "        bars.append(bar[0])\n",
    "\n",
    "    # Draw legend if we need\n",
    "    if legend:\n",
    "        ax.legend(bars, data.keys())\n",
    "    \n",
    "    ax.set_ylabel('Accuracy',fontsize=15)\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(lambda y,_: \"{:.0%}\".format(y)))\n",
    "    ind=np.arange(len(data[list(data.keys())[0]]))\n",
    "    ax.set_xticks(ind)\n",
    "    ax.set_xticklabels( ('top 1% score', 'top 2% score', 'top 5% score','top 10% score') )\n",
    "    ax.set_title(f\"Top Predicted Score  \",fontsize=15)\n",
    "    \n",
    "    #     plt.xlim([0, 1])\n",
    "    # plt.ylim([axis_truncation, 1])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d84434-7f48-4db6-aa78-726ef811cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_read(table_name):\n",
    "    df=pd.read_csv(table_name)\n",
    "    true_y=df[\"True label\"].values\n",
    "    pred_y=df[\"Predicted label\"].values\n",
    "    prob_y=df[\"Predicted_prob\"].values\n",
    "    return true_y.tolist(), prob_y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6d46a-6677-4b52-8fad-db46683c1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large\"\n",
    "table_name=os.path.join(os.getcwd(),model_name,\"predictions.csv\")\n",
    "pretrained_lf_true, pretrained_lf_prob=table_read(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7f9582-357b-44f0-a82a-0475c16d38f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large_customized\"\n",
    "table_name=os.path.join(os.getcwd(),model_name,\"predictions.csv\")\n",
    "customized_lf_true, customized_lf_prob=table_read(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c730af5-5878-44a0-9a69-a2c2213f5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk=[0.01,0.02,0.05,0.1]\n",
    "\n",
    "response_v0 = response_rate_eval(pretrained_lf_prob,pretrained_lf_true, topk)\n",
    "response_v1 = response_rate_eval(customized_lf_prob,customized_lf_true, topk)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = {\n",
    "        \"pretrained-longformer\": response_v0,\n",
    "        \"customized-longformer\": response_v1\n",
    "        \n",
    "    }\n",
    "\n",
    "    \n",
    "    CL=['r', 'g', 'b', 'c', 'y', 'darkorange', 'lime', 'grey','gold','bisque', 'lightseagreen', 'purple']\n",
    "    bar_plot(data, colors=CL,total_width=.7, single_width=1,title=\"(response rate)\",subtitle=\"Test Set \",axis_truncation=0.50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8b4e5-98e4-4b1c-8b08-351d37292f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146eba73-f0a8-44c5-b7e9-d71f59368588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959fe34-91a4-4c53-b61f-7fb187aa0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887a469b-ecb7-47c9-8c97-2039ad96c9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"longformer_large\"\n",
    "table_name=os.path.join(os.getcwd(),model_name,\"predictions.csv\")\n",
    "pretrained_lf_true, pretrained_lf_prob=table_read(table_name)\n",
    "\n",
    "model_name=\"longformer_large_customized\"\n",
    "table_name=os.path.join(os.getcwd(),model_name,\"predictions.csv\")\n",
    "customized_lf_true, customized_lf_prob=table_read(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b740dc-ffd7-4e09-989c-119dee995d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_lf_fpr, pretrained_lf_tpr, _ = roc_curve(pretrained_lf_true,  pretrained_lf_prob)\n",
    "customized_lf_fpr, customized_lf_tpr, _ = roc_curve(customized_lf_true,  customized_lf_prob)\n",
    "\n",
    "fig = plt.subplots(nrows=1,ncols=1,figsize =(6, 4))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.plot(pretrained_lf_fpr, pretrained_lf_tpr, linestyle='solid', label='pretrained-longformer', color ='purple', linewidth=2)\n",
    "plt.plot(customized_lf_fpr, customized_lf_tpr, linestyle='solid', label='customized-longformer', color ='red', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], linestyle='solid', label='random model', color ='darkorange', linewidth=2)\n",
    "plt.xlabel('False Positive Rate', fontweight ='bold',fontsize=15)\n",
    "plt.ylabel('True Positive Rate', fontweight ='bold',fontsize=15)\n",
    "plt.title(f'ROC AUC CURVE', fontweight ='bold',fontsize=18)\n",
    "plt.xlim([0, 1.01])\n",
    "plt.ylim([0, 1.01])\n",
    "plt.legend(fontsize=\"large\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e8450-7a83-43e8-9de5-d194de0986d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
