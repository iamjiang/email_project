{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41dfd9b-88d8-4bce-9ee8-98ad7f347cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savez_compressed, load\n",
    "import itertools\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset, concatenate_datasets,DatasetDict\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(position=0,leave=True)\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Load the stopwords from the new directory\n",
    "nltk_data_dir=os.path.join(\"/opt/omniai/work/instance1/jupyter/\", \"transformers-models\",\"nltk_data\")\n",
    "stopwords_file = open(nltk_data_dir + '/corpora/stopwords/english')\n",
    "stopwords_list = stopwords_file.readlines()\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "import spacy\n",
    "model_name=os.path.join(\"/opt/omniai/work/instance1/jupyter/\", \"transformers-models\",\"en_core_web_md\",\"en_core_web_md-3.3.0\")\n",
    "nlp = spacy.load(model_name)\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "sns.set(style=\"whitegrid\",palette='muted',font_scale=1.2)\n",
    "rcParams['figure.figsize']=16,10\n",
    "\n",
    "%config InlineBackend.figure_format=\"retina\"\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None,'display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9312873-da0e-4a50-a46c-f471d51f13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"/opt/omniai/work/instance1/jupyter/v2_new_email/datasets/raw_data\"\n",
    "data_name=[x for x in os.listdir(root_dir) if x.split(\".\")[-1]==\"csv\"]\n",
    "df=pd.DataFrame()\n",
    "for data in data_name:\n",
    "    x=pd.read_csv(os.path.join(root_dir,data))\n",
    "    x=x.dropna(subset=['email'])\n",
    "    x=x[x.email.notna()]\n",
    "    x=x[x.email.str.len()>0]\n",
    "    df=pd.concat([df,x],axis=0,ignore_index=True)\n",
    "    print(\"{:<20}{:<20,}\".format(data.split(\"_\")[2],x.shape[0]))\n",
    "    \n",
    "df=df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea25d5-cc6b-4085-82b8-b6481f23b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['year'] = df.time.apply(lambda x: x.year)\n",
    "df['month'] = df.time.apply(lambda x: x.month)\n",
    "df['day'] = df.time.apply(lambda x: x.day)\n",
    "df.sort_values(by='time', inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4611fc8-46b9-4f05-b9b3-722f0cb0fd32",
   "metadata": {},
   "source": [
    "#### remove duplicated emails based on thread id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f194ee8-e9b1-4c88-ab28-8cd2c0d3283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df=df.groupby('thread_id')\n",
    "sorted_groups=[group.sort_values(\"time\",ascending=False).reset_index(drop=True) for _, group in grouped_df]\n",
    "df=pd.concat(sorted_groups).drop_duplicates(subset=\"thread_id\", keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a344158-b8e4-4765-999e-a7c699298081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(df,year,month):\n",
    "    df=df[(df.year==year) & (df.month==month)]\n",
    "    tempt1=pd.DataFrame(df[\"is_complaint\"].value_counts(dropna=False)).reset_index().rename(columns={'index':'is_complaint','is_complaint':'count'})\n",
    "    tempt2=pd.DataFrame(df[\"is_complaint\"].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':'is_complaint','is_complaint':'percentage'})\n",
    "    tempt3=tempt1.merge(tempt2, on=\"is_complaint\", how=\"inner\")\n",
    "    tempt3['year']=year\n",
    "    tempt3['month']=month\n",
    "    tempt3=tempt3.loc[:,['year','month','is_complaint','count','percentage']]\n",
    "    return tempt3\n",
    "\n",
    "def style_format(df,  data_type=\"Training set\"):\n",
    "    return df.style.format({'count':'{:,}','percentage':'{:.2%}'})\\\n",
    "           .set_caption(f\"label distribution\\n{data_type}\")\\\n",
    "           .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d009f-c340-4fda-877b-8cdfd94b02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df=pd.DataFrame()\n",
    "# dist_df=pd.concat([dist_df,label_distribution(df,2022,8)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,9)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,10)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,11)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,12)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,1)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,2)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,3)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,4)])\n",
    "style_format(dist_df,  data_type=\"split by month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05112f0-b826-4937-b1dd-e0fadf786228",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=df['state'].value_counts(dropna=False).reset_index(name=\"count\")\n",
    "tempt2=df['state'].value_counts(dropna=False,normalize=True).reset_index(name=\"percentage\")\n",
    "tempt3=tempt1.merge(tempt2, on=['index'], how=\"inner\")\n",
    "tempt3.rename(columns={\"index\":\"state\"}, inplace=True)\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be2e00-3704-4eb1-9c05-6c53a4d3cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df.state==\"closed\"]\n",
    "dist_df=pd.DataFrame()\n",
    "# dist_df=pd.concat([dist_df,label_distribution(df,2022,8)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,9)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,10)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,11)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2022,12)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,1)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,2)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,3)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df,2023,4)])\n",
    "style_format(dist_df,  data_type=\"split by month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6842fc32-49f2-42f1-b88e-834bf85af0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train: 09/2022 ~ 02/2023. validation: 03/2023  test: 04/2023\n",
    "set_categories=lambda row: \"train\" if (row[\"year\"] in [2022,2023] and row[\"month\"] in [9,10,11,12,1,2]) \\\n",
    "else (\"val\" if (row[\"year\"]==2023 and row[\"month\"]==3) else \"test\")\n",
    "\n",
    "df[\"data_type\"]=df.progress_apply(set_categories,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43a2db8-2519-4206-81b3-de65c838993a",
   "metadata": {},
   "source": [
    "### After data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8b3954-7e0f-4837-92eb-ef6d90bf6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"/opt/omniai/work/instance1/jupyter/v2_new_email/datasets/split_data\"\n",
    "data_name=[x for x in os.listdir(root_dir) if x.split(\"_\")[-2]==\"pickle\"]\n",
    "df1=pd.DataFrame()\n",
    "for data in data_name:\n",
    "    x=pd.read_pickle(os.path.join(root_dir,data))\n",
    "    x=x.dropna(subset=['email'])\n",
    "    x=x[x.email.notna()]\n",
    "    x=x[x.email.str.len()>0]\n",
    "    df1=pd.concat([df1,x],axis=0,ignore_index=True)\n",
    "    # print(\"{:<20}{:<20,}\".format(data.split(\"_\")[2],x.shape[0]))\n",
    "    \n",
    "df1=df1.reset_index(drop=True)\n",
    "\n",
    "df1['time'] = pd.to_datetime(df1['time'])\n",
    "df1['year'] = df1.time.apply(lambda x: x.year)\n",
    "df1['month'] = df1.time.apply(lambda x: x.month)\n",
    "df1['day'] = df1.time.apply(lambda x: x.day)\n",
    "df1.sort_values(by='time', inplace = True) \n",
    "\n",
    "grouped_df=df1.groupby('thread_id')\n",
    "sorted_groups=[group.sort_values(\"time\",ascending=False).reset_index(drop=True) for _, group in grouped_df]\n",
    "df1=pd.concat(sorted_groups).drop_duplicates(subset=\"thread_id\", keep=\"first\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ee0db-5d23-4752-bc17-e25eca7b0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df=pd.DataFrame()\n",
    "# dist_df=pd.concat([dist_df,label_distribution(df1,2022,8)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,9)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,10)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,11)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,12)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,1)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,2)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,3)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,4)])\n",
    "style_format(dist_df,  data_type=\"split by month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615a779-4371-4330-adb8-9f01c9ad0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempt1=df1['state'].value_counts(dropna=False).reset_index(name=\"count\")\n",
    "tempt2=df1['state'].value_counts(dropna=False,normalize=True).reset_index(name=\"percentage\")\n",
    "tempt3=tempt1.merge(tempt2, on=['index'], how=\"inner\")\n",
    "tempt3.rename(columns={\"index\":\"state\"}, inplace=True)\n",
    "tempt3.style.format({'count':'{:,}','percentage':'{:.2%}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4748a8a-f5e5-4ae0-b3aa-e2e568941453",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1[df1.state==\"closed\"]\n",
    "dist_df=pd.DataFrame()\n",
    "# dist_df=pd.concat([dist_df,label_distribution(df1,2022,8)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,9)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,10)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,11)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2022,12)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,1)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,2)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,3)])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,2023,4)])\n",
    "style_format(dist_df,  data_type=\"split by month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27a432b-6812-49f4-8af0-94384deacb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_test_month=pd.DataFrame({\"split\":[\"train\",\"val\",\"test\"],\\\n",
    "                                   \"month\":[\"09/2022 ~ 02/2023\",\"03/2023\",\"04/2023\"]})\n",
    "train_val_test_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05825d-b598-4f44-966d-873965352392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(df,data_type):\n",
    "    df=df[df[\"data_type\"]==data_type]\n",
    "    tempt1=pd.DataFrame(df[\"is_complaint\"].value_counts(dropna=False)).reset_index().rename(columns={'index':'is_complaint','is_complaint':'count'})\n",
    "    tempt2=pd.DataFrame(df[\"is_complaint\"].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':'is_complaint','is_complaint':'percentage'})\n",
    "    tempt3=tempt1.merge(tempt2, on=\"is_complaint\", how=\"inner\")\n",
    "    tempt3['data_type']=data_type\n",
    "    tempt3=tempt3.loc[:,['data_type','is_complaint','count','percentage']]\n",
    "    return tempt3\n",
    "\n",
    "def style_format(df):\n",
    "    return df.style.format({'count':'{:,}','percentage':'{:.2%}'})\\\n",
    "           .set_caption(f\"label distribution\")\\\n",
    "           .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef4acb-844f-44df-8a2d-4ae80f025836",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df=pd.DataFrame()\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,\"train\")])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,\"val\")])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df1,\"test\")])\n",
    "style_format(dist_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d5d18a-e009-4080-ac8a-8f727acd67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedback_distribution(df):\n",
    "    tempt1=df.groupby('is_feedback')['is_complaint'].value_counts(dropna=False).reset_index(name=\"count\")\n",
    "    tempt2=df.groupby('is_feedback')['is_complaint'].value_counts(dropna=False,normalize=True).reset_index(name=\"percentage\")\n",
    "    tempt3=tempt1.merge(tempt2, on=['is_feedback',\"is_complaint\"], how=\"inner\")\n",
    "    tempt3=tempt3.loc[:,['is_feedback','is_complaint','count','percentage']]\n",
    "    return tempt3\n",
    "\n",
    "def style_format(df):\n",
    "    return df.style.format({'count':'{:,}','percentage':'{:.2%}'})\\\n",
    "           .set_caption(f\"feedback distribution\")\\\n",
    "           .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])\n",
    "\n",
    "dist_df=feedback_distribution(df1)\n",
    "style_format(dist_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc172ec-f67d-4bdd-b4b2-796607bf1e75",
   "metadata": {},
   "source": [
    "### email text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea0db7-2cd0-4d7e-b67d-e7e857eb89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset, concatenate_datasets,DatasetDict\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(position=0,leave=True)\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name=\"longformer-base-4096\"\n",
    "model_path=os.path.join(\"/opt/omniai/work/instance1/jupyter/\", \"transformers-models\",model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "print()\n",
    "print(f\"Vocabulary size : {tokenizer.vocab_size:,}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04f217f-193a-47c3-8cde-e64e5d4a5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_hf(df):\n",
    "   \n",
    "    train_df=df[df[\"data_type\"]==\"train\"]\n",
    "    val_df=df[df[\"data_type\"]==\"val\"]\n",
    "    test_df=df[df[\"data_type\"]==\"test\"]\n",
    "    \n",
    "    hf_train=Dataset.from_pandas(train_df)\n",
    "    hf_val=Dataset.from_pandas(val_df)\n",
    "    hf_test=Dataset.from_pandas(test_df)\n",
    "    \n",
    "    hf_data=DatasetDict({\"train\":hf_train, \"val\":hf_val,  \"test\":hf_test})\n",
    "    # hf_data=hf_data.select_columns(['snapshot_id','thread_id','time','preprocessed_email','is_feedback','is_complaint'])\n",
    "    \n",
    "    return hf_data\n",
    "\n",
    "hf_v0=dataframe_hf(df)\n",
    "hf_v1=dataframe_hf(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dba364-17ac-404b-934a-93f467977fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lenth(example):\n",
    "    return {\"text_length\":len(example[\"input_ids\"])}\n",
    "\n",
    "hf_v0=hf_v0.map(lambda x: tokenizer(x[\"email\"]),batched=True)\n",
    "hf_v0=hf_v0.map(compute_lenth)\n",
    "\n",
    "hf_v1=hf_v1.map(lambda x: tokenizer(x[\"preprocessed_email\"]),batched=True)\n",
    "hf_v1=hf_v1.map(compute_lenth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06f5967-7afa-40fc-b764-6e1ceb2ff575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_compute(hf_df1,hf_df2,hf_df3,p=1):\n",
    "\n",
    "    X=[]\n",
    "    X.append(np.percentile(hf_df1['text_length'],p))\n",
    "    X.append(np.percentile(hf_df2['text_length'],p))\n",
    "    X.append(np.percentile(hf_df3['text_length'],p))\n",
    "    \n",
    "    result={}\n",
    "    result['percentile']=X\n",
    "    result[\"min\"]=[np.min(hf_df1['text_length']),np.min(hf_df2['text_length']),np.min(hf_df3['text_length'])]\n",
    "    result[\"max\"]=[np.max(hf_df1['text_length']),np.max(hf_df2['text_length']),np.max(hf_df3['text_length'])]\n",
    "    result[\"mean\"]=[np.mean(hf_df1['text_length']),np.mean(hf_df2['text_length']),np.mean(hf_df3['text_length'])]\n",
    "    return result\n",
    "\n",
    "def statistics_table(hf_df1,hf_df2,hf_df3):\n",
    "    dict_data={}\n",
    "    dict_data[\"data_type\"]=[\"training\", \"validation\", \"test\"]\n",
    "    dict_data[\"# of obs\"]=[len(hf_df1['text_length']),len(hf_df2['text_length']),len(hf_df3['text_length'])]\n",
    "    dict_data[\"Min of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3)[\"min\"]\n",
    "    dict_data[\"1% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=1)['percentile']\n",
    "    dict_data[\"5% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=5)['percentile']\n",
    "    dict_data[\"10% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=10)['percentile']\n",
    "    dict_data[\"25% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=25)['percentile']\n",
    "    dict_data[\"Median of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=50)['percentile']\n",
    "    dict_data[\"Average tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3)[\"mean\"]\n",
    "    dict_data[\"75% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=75)['percentile']\n",
    "    dict_data[\"90% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=90)['percentile']\n",
    "    dict_data[\"95% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=95)['percentile']\n",
    "    dict_data[\"99% of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3, p=99)['percentile']\n",
    "    dict_data[\"Max of tokens\"]=statistics_compute(hf_df1, hf_df2, hf_df3)[\"max\"]\n",
    "    token_count_df=pd.DataFrame(dict_data)\n",
    "    return token_count_df\n",
    "\n",
    "def style_format(token_count_df,  textbody=\"preprocessed_email\"):\n",
    "    token_count_df=token_count_df.set_index(\"data_type\")\n",
    "    token_count_df[list(token_count_df.columns)] = token_count_df[list(token_count_df.columns)].astype(int)\n",
    "    return token_count_df.style.format(\"{:,}\").set_caption(f\"Summary Statistics of token lengths for {textbody} \").set_table_styles([{\n",
    "        'selector': 'caption',\n",
    "        'props': [\n",
    "            ('color', 'red'),\n",
    "            ('font-size', '15px')\n",
    "        ]\n",
    "    }])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5259e-09c0-468d-a57d-95e6c89f8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_df=statistics_table(hf_v0[\"train\"],hf_v0[\"val\"],hf_v0[\"test\"])\n",
    "style_format(token_count_df,  textbody=\"email data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e4bf8-cf77-4558-8389-af6dd5471a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_df=statistics_table(hf_v1[\"train\"],hf_v1[\"val\"],hf_v1[\"test\"])\n",
    "style_format(token_count_df,  textbody=\"preprocessed email\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9db4d4-48b9-4911-9a6d-5302069bb085",
   "metadata": {},
   "source": [
    "### text length distribution for complaint email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec4399-ac2c-48a1-8483-0b6ed6e78de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_df=statistics_table(hf_v1[\"train\"].filter(lambda x : x[\"is_complaint\"]==\"Y\"), \\\n",
    "                                hf_v1[\"val\"].filter(lambda x : x[\"is_complaint\"]==\"Y\"),\\\n",
    "                                hf_v1[\"test\"].filter(lambda x : x[\"is_complaint\"]==\"Y\"))\n",
    "style_format(token_count_df,  textbody=\"Complaint email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc1aa4a-c58c-4439-a22d-ee6e71a72a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_count_df=statistics_table(hf_v1[\"train\"].filter(lambda x : x[\"is_complaint\"]==\"N\"), \\\n",
    "                                hf_v1[\"val\"].filter(lambda x : x[\"is_complaint\"]==\"N\"),\\\n",
    "                                hf_v1[\"test\"].filter(lambda x : x[\"is_complaint\"]==\"N\"))\n",
    "style_format(token_count_df,  textbody=\"Non-complaint email\")b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9390e18-a971-4228-996d-878158a7120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcut_func(df,var,nbin=5):\n",
    "    df[var]=df[var].astype(float)\n",
    "    df[\"cut\"]=pd.qcut(df[var],nbin,precision=2,duplicates=\"drop\")\n",
    "    decile=df.groupby(df[\"cut\"])['target'].mean().reset_index()\n",
    "    decile[\"cut\"]=decile[\"cut\"].astype(str)\n",
    "    return decile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c5b0a-d394-4120-a18e-f089bc293f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=hf_v1[\"train\"]\n",
    "val_df=hf_v1[\"val\"]\n",
    "test_df=hf_v1[\"test\"]\n",
    "\n",
    "train_df.set_format(\"pandas\")\n",
    "df_train=train_df[:]\n",
    "df_train[\"target\"]=df_train['is_complaint'].apply(lambda x : 1 if x==\"Y\" else 0)\n",
    "\n",
    "val_df.set_format(\"pandas\")\n",
    "df_val=val_df[:]\n",
    "df_val[\"target\"]=df_val['is_complaint'].apply(lambda x : 1 if x==\"Y\" else 0)\n",
    "\n",
    "test_df.set_format(\"pandas\")\n",
    "df_test=test_df[:]\n",
    "df_test[\"target\"]=df_test['is_complaint'].apply(lambda x : 1 if x==\"Y\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1044ef-d9d0-466c-be04-8e20c5c62bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def y_formatter(x,_):\n",
    "    return f'{x*100:.2f}%'\n",
    "fig, ax = plt.subplots(1,3,figsize=(15,6))\n",
    "plt.subplot(1,3,1)\n",
    "df=pcut_func(df_train,var=\"text_length\",nbin=10)\n",
    "ax[0].plot(df[\"cut\"],df[\"target\"],color=\"r\",marker=\"*\",linewidth=2, markersize=12)\n",
    "ax[0].set_title(\"text_length\\n(training set)\")\n",
    "ax[0].set_ylabel(\"complaint %\")\n",
    "ax[0].tick_params(labelrotation=45)\n",
    "ax[0].yaxis.set_major_formatter(ticker.FuncFormatter(y_formatter))\n",
    "plt.subplot(1,3,2)\n",
    "df=pcut_func(df_val,var=\"text_length\",nbin=10)\n",
    "ax[1].plot(df[\"cut\"],df[\"target\"],color=\"r\",marker=\"*\",linewidth=2, markersize=12)\n",
    "ax[1].set_title(\"text_length\\n(validation set)\")\n",
    "ax[1].set_ylabel(\"complaint %\")\n",
    "ax[1].tick_params(labelrotation=45)\n",
    "ax[1].yaxis.set_major_formatter(ticker.FuncFormatter(y_formatter))\n",
    "plt.subplot(1,3,3)\n",
    "df=pcut_func(df_test,var=\"text_length\",nbin=10)\n",
    "ax[2].plot(df[\"cut\"],df[\"target\"],color=\"r\",marker=\"*\",linewidth=2, markersize=12)\n",
    "ax[2].set_title(\"text_length\\n(test set)\")\n",
    "ax[2].set_ylabel(\"complaint %\")\n",
    "ax[2].tick_params(labelrotation=45)\n",
    "ax[2].yaxis.set_major_formatter(ticker.FuncFormatter(y_formatter))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb5b9a-5ea9-4be1-8127-ff320672c33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d062d67b-d84d-4191-859c-e67262349bd8",
   "metadata": {},
   "source": [
    "### short and long email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf4a81-e8cd-4c52-aced-a56512085c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=\"/opt/omniai/work/instance1/jupyter/v2_new_email/datasets/split_data\"\n",
    "data_name=[x for x in os.listdir(root_dir) if x.split(\"_\")[-2]==\"pickle\"]\n",
    "df1=pd.DataFrame()\n",
    "for data in data_name:\n",
    "    x=pd.read_pickle(os.path.join(root_dir,data))\n",
    "    x=x.dropna(subset=['email'])\n",
    "    x=x[x.email.notna()]\n",
    "    x=x[x.email.str.len()>0]\n",
    "    df1=pd.concat([df1,x],axis=0,ignore_index=True)\n",
    "    # print(\"{:<20}{:<20,}\".format(data.split(\"_\")[2],x.shape[0]))\n",
    "    \n",
    "df1=df1.reset_index(drop=True)\n",
    "\n",
    "df1['time'] = pd.to_datetime(df1['time'])\n",
    "df1['year'] = df1.time.apply(lambda x: x.year)\n",
    "df1['month'] = df1.time.apply(lambda x: x.month)\n",
    "df1['day'] = df1.time.apply(lambda x: x.day)\n",
    "df1.sort_values(by='time', inplace = True) \n",
    "\n",
    "### only keep emails with status=closed\n",
    "df1=df1[df1.state==\"closed\"]\n",
    "\n",
    "## train: 09/2022 ~ 02/2023. validation: 03/2023  test: 04/2023\n",
    "set_categories=lambda row: \"train\" if (row[\"year\"] in [2022,2023] and row[\"month\"] in [9,10,11,12,1,2]) \\\n",
    "else (\"val\" if (row[\"year\"]==2023 and row[\"month\"]==3) else \"test\")\n",
    "df1[\"data_type\"]=df1.progress_apply(set_categories,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7eecf5-2c32-4e11-8fbe-4c8030970c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"long_short\"]=df1['text_length'].progress_apply(lambda x : 1 if x>512 else 0)\n",
    "df_short=df1[df1[\"long_short\"]==0]\n",
    "df_long=df1[df1[\"long_short\"]==1]\n",
    "\n",
    "df_short.drop(\"long_short\", axis=1, inplace=True)\n",
    "df_short=df_short.reset_index(drop=True)\n",
    "\n",
    "df_long.drop(\"long_short\", axis=1, inplace=True)\n",
    "df_long=df_long.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c7ecb2-5d97-406b-866e-37d7c8f6c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_distribution(df,data_type):\n",
    "    df=df[df[\"data_type\"]==data_type]\n",
    "    tempt1=pd.DataFrame(df[\"is_complaint\"].value_counts(dropna=False)).reset_index().rename(columns={'index':'is_complaint','is_complaint':'count'})\n",
    "    tempt2=pd.DataFrame(df[\"is_complaint\"].value_counts(dropna=False,normalize=True)).reset_index().rename(columns={'index':'is_complaint','is_complaint':'percentage'})\n",
    "    tempt3=tempt1.merge(tempt2, on=\"is_complaint\", how=\"inner\")\n",
    "    tempt3['data_type']=data_type\n",
    "    tempt3=tempt3.loc[:,['data_type','is_complaint','count','percentage']]\n",
    "    return tempt3\n",
    "\n",
    "def style_format(df, title):\n",
    "    return df.style.format({'count':'{:,}','percentage':'{:.2%}'})\\\n",
    "           .set_caption(f\"{title}\")\\\n",
    "           .set_table_styles([{'selector': 'caption','props': [('color', 'red'),('font-size', '15px')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62ec18-0bc7-4a5d-8921-e7a2b9ab942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df=pd.DataFrame()\n",
    "dist_df=pd.concat([dist_df,label_distribution(df_short,\"train\")])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df_short,\"val\")])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df_short,\"test\")])\n",
    "style_format(dist_df,title=f\"label distribution for short email\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa646e9-dc65-4e30-93fd-5fc5fdbf1103",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df=pd.DataFrame()\n",
    "dist_df=pd.concat([dist_df,label_distribution(df_long,\"train\")])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df_long,\"val\")])\n",
    "dist_df=pd.concat([dist_df,label_distribution(df_long,\"test\")])\n",
    "style_format(dist_df,title=\"label distribution for long email\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
